{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "from SAC_beta.replaybuffer import ReplayBuffers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.h1 = nn.Linear(state_dim, 128)\n",
    "        self.h2 = nn.Linear(128, 64)\n",
    "        self.h3 = nn.Linear(64, 32)\n",
    "        self.h4 = nn.Linear(32, 16)\n",
    "        self.mu = nn.Linear(16, action_dim)\n",
    "        self.std = nn.Linear(16, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = nn.functional.relu(self.h1(state))\n",
    "        x = nn.functional.relu(self.h2(x))\n",
    "        x = nn.functional.relu(self.h3(x))\n",
    "        x = nn.functional.relu(self.h4(x))\n",
    "        mu = torch.tanh(self.mu(x))\n",
    "        std = nn.functional.softplus(self.std(x))\n",
    "\n",
    "        std = torch.clamp(std, self.std_bound[0], self.std_bound[1])\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "    def sample_normal(self, mu, std):\n",
    "        normal_prob = Normal(mu, std)\n",
    "        action = normal_prob.sample()\n",
    "\n",
    "        # limit the action value\n",
    "        log_prob = normal_prob.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.x1 = nn.Linear(state_dim, 128)\n",
    "        self.a1 = nn.Linear(action_dim, 128)\n",
    "        # this layer is responsible for taking mixed state_action len.\n",
    "        self.h = nn.Sequential(\n",
    "            # 256 because it will be connected to two input tensors\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        state = state_action[0]\n",
    "        action = state_action[1]\n",
    "        x = nn.functional.relu(self.x1(state))\n",
    "        a = nn.functional.relu(self.a1(action))\n",
    "        h = torch.cat((x, a), dim=-1)\n",
    "        q = self.h(h)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SACagent(object):\n",
    "    def __init__(self, N_STATES, N_ACTIONS):\n",
    "        # Hyperparameters\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 1000\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "        self.ALPHA = 0.5\n",
    "\n",
    "        # Observation space and Action space\n",
    "        self.state_dim = N_STATES\n",
    "        self.action_dim = N_ACTIONS\n",
    "\n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Build Actor, Q1, Q2 and their target networks\n",
    "        self.actor = Actor(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "\n",
    "        self.critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        self.critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        # self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        # self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.ACTOR_LEARNING_RATE)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # Clear out the buffer\n",
    "        self.buffer = ReplayBuffers(self.BUFFER_SIZE)\n",
    "\n",
    "        # For plotting purposes, data is stored.\n",
    "        self.policy_loss = []\n",
    "        self.reward_list = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "            mu, std = self.actor(state)\n",
    "            normal = torch.distributions.Normal(mu, std)\n",
    "            action = normal.sample()\n",
    "            return action.cpu().numpy()\n",
    "\n",
    "    def update_target_network(self, TAU):\n",
    "        phi_1 = self.critic_1.state_dict()\n",
    "        phi_2 = self.critic_2.state_dict()\n",
    "        target_phi_1 = self.target_critic_1.state_dict()\n",
    "        target_phi_2 = self.target_critic_2.state_dict()\n",
    "        for name in phi_1:\n",
    "            target_phi_1[name] = TAU * phi_1[name] + (1 - TAU) * target_phi_1[name]\n",
    "            target_phi_2[name] = TAU * phi_2[name] + (1 - TAU) * target_phi_2[name]\n",
    "        self.target_critic_1.load_state_dict(target_phi_1)\n",
    "        self.target_critic_2.load_state_dict(target_phi_2)\n",
    "\n",
    "\n",
    "    # train Q1, Q2\n",
    "    def critic_learn(self, states, actions, y_i):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        q_targets = y_i.to(self.device)\n",
    "        q_1 = self.critic_1([states, actions])\n",
    "        # where q_1 is the predicted value, q_targets is the true val.\n",
    "        loss_1 = F.mse_loss(q_1, q_targets)\n",
    "\n",
    "        # sets the gradients of all parameters of the optimizer to zero. This is necessary to prevent the gradients from accumulating from multiple backpropagation passes.\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        #computes the gradients of the loss with respect to all the learnable parameters in the critic network\n",
    "        loss_1.backward()\n",
    "        #updates the parameters of the critic network based on the computed gradients.\n",
    "        self.critic_1_optimizer.step()\n",
    "\n",
    "        q_2 = self.critic_2([states, actions])\n",
    "        loss_2 = F.mse_loss(q_2, q_targets)\n",
    "\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "    def actor_learn(self, states):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        mu, std = self.actor(states)\n",
    "        actions, log_pdfs = self.actor.sample_normal(mu, std)\n",
    "        log_pdfs = log_pdfs.squeeze(1)\n",
    "        soft_q_1 = self.critic_1([states, actions])\n",
    "        soft_q_2 = self.critic_2([states, actions])\n",
    "        soft_q = torch.min(soft_q_1, soft_q_2)\n",
    "\n",
    "        loss = torch.mean(self.ALPHA * log_pdfs - soft_q)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return float(loss)\n",
    "\n",
    "    def q_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return torch.tensor(y_k, dtype=torch.float32)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path + 'Drone_actor_2q.pth'))\n",
    "        self.critic_1.load_state_dict(torch.load(path + 'Drone_critic_12q.pth'))\n",
    "        self.critic_2.load_state_dict(torch.load(path + 'Drone_critic_22q.pth'))\n",
    "\n",
    "    def train(self, max_episode_num, env, behavior_name):\n",
    "        self.param_counter = 0\n",
    "        cnt = 0\n",
    "        # reset target network param.\n",
    "        self.update_target_network(1.0)\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            frame, episode_reward = 0, 0\n",
    "            # reset the enviornment\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            episode_done = False\n",
    "            # setting up the initial state as an array\n",
    "            x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "            y = decision_steps.obs[1][0] # Agent's velocity x,z\n",
    "            state = np.concatenate((x, y), 0)\n",
    "\n",
    "            while not episode_done:\n",
    "\n",
    "                action = self.get_action(state)\n",
    "                # wrap the action with ActionTuple before sending it to UE.\n",
    "                action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "                env.set_actions(behavior_name, action)\n",
    "                # move the agent along with the action.\n",
    "                env.step()\n",
    "                action = action._continuous # converting ActionTuple to array\n",
    "                next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # if the agent is still on, collect data and add it to buffer.\n",
    "                if next_decision_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_decision_steps.reward[0]\n",
    "                    x = next_decision_steps.obs[0][0]\n",
    "                    y = next_decision_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    episode_reward += next_decision_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, False)\n",
    "                    episode_done = False\n",
    "\n",
    "                # if the agent is off, collect data and add True for done.\n",
    "                if next_terminal_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_terminal_steps.reward[0]\n",
    "                    x = next_terminal_steps.obs[0][0]\n",
    "                    y = next_terminal_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    episode_reward += next_terminal_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, True)\n",
    "                    episode_done = True\n",
    "\n",
    "                # if buffer has enough data start training.\n",
    "                if self.buffer.buffer_count() > self.BATCH_SIZE:\n",
    "\n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "\n",
    "                    # Calculate the Q target value\n",
    "                    with torch.no_grad():\n",
    "                        next_mu, next_std = self.actor(torch.tensor(next_states, dtype=torch.float32))\n",
    "                        next_actions, next_log_pdf = self.actor.sample_normal(next_mu, next_std)\n",
    "\n",
    "                        # convert np to tensor\n",
    "                        tensor_next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                        tensor_next_actions = torch.tensor(next_actions, dtype=torch.float32)\n",
    "\n",
    "                        # move to CUDA\n",
    "                        tensor_next_states = tensor_next_states.to(self.device)\n",
    "                        tensor_next_actions = tensor_next_actions.to(self.device)\n",
    "\n",
    "                        target_qs_1 = self.target_critic_1([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs_2 = self.target_critic_2([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs = torch.min(target_qs_1, target_qs_2)\n",
    "\n",
    "                        target_qi = target_qs - self.ALPHA * next_log_pdf\n",
    "                        y_i = self.q_target(rewards, target_qi.numpy(), dones)\n",
    "\n",
    "\n",
    "                    self.critic_learn(states, actions, y_i)\n",
    "\n",
    "                    # update Actor and return policy loss\n",
    "                    policy_loss = self.actor_learn(states)\n",
    "\n",
    "                    # store the performance of the algorithm.\n",
    "                    if cnt % 500 == 0:\n",
    "                        self.reward_list.append(train_reward)\n",
    "                        self.policy_loss.append(policy_loss)\n",
    "\n",
    "                    self.update_target_network(self.TAU)\n",
    "\n",
    "                state = next_state\n",
    "                frame += 1\n",
    "                cnt += 1\n",
    "\n",
    "            # Episode output\n",
    "            print('Episode: ', ep+1, 'Frame: ', frame, 'u Reward: ', episode_reward)\n",
    "\n",
    "            # every 250th run will store another params\n",
    "            if max_episode_num % 250:\n",
    "                torch.save(self.actor.state_dict(), \"./saved_weights/250th/actor_\"+self.param_counter+\"_2q.pth\")\n",
    "                torch.save(self.critic_1.state_dict(), \"./saved_weights/250th/critic_\"+self.param_counter+\"_12q.pth\")\n",
    "                torch.save(self.critic_2.state_dict(), \"./saved_weights/250th/critic_\"+self.param_counter+\"_22q.pth\")\n",
    "                self.param_counter += 1\n",
    "            # Save weights for each run\n",
    "            torch.save(self.actor.state_dict(), \"./saved_weights/Drone_actor_2q.pth\")\n",
    "            torch.save(self.critic_1.state_dict(), \"./saved_weights/Drone_critic_12q.pth\")\n",
    "            torch.save(self.critic_2.state_dict(), \"./saved_weights/Drone_critic_22q.pth\")\n",
    "\n",
    "    def plot_result(self):\n",
    "        fig=plt.figure(figsize=(18, 6))\n",
    "        fig.add_subplot(1, 3, 1)  # 1 row, 3 columns\n",
    "        plt.plot(self.reward_list)\n",
    "\n",
    "        fig.add_subplot(1, 3, 3)\n",
    "        plt.plot(self.policy_loss)\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Run Unity Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ACTIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= \"./LinuxBuilds/Linux_Drone_v0.0.1/Linux_Drone_with.x86_64\", base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial states\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1 Frame:  145 u Reward:  -0.013421893941945044\n",
      "Episode:  2 Frame:  130 u Reward:  -0.014001699594350962\n",
      "Episode:  3 Frame:  120 u Reward:  -0.017528661092122397\n",
      "Episode:  4 Frame:  135 u Reward:  -0.007852568449797453\n",
      "Episode:  5 Frame:  180 u Reward:  -0.009620157877604167\n",
      "Episode:  6 Frame:  89 u Reward:  -0.024540933330407303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/11k7rc0x5xl576ckk4jkh6ch0000gn/T/ipykernel_14231/2943630160.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_next_actions = torch.tensor(next_actions, dtype=torch.float32)\n",
      "/var/folders/bx/11k7rc0x5xl576ckk4jkh6ch0000gn/T/ipykernel_14231/2943630160.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_i, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  7 Frame:  301 u Reward:  -0.019364886109615086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[234], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m SACagent(N_STATES, N_ACTIONS)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# usually 30K is enough.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[228], line 187\u001b[0m, in \u001b[0;36mSACagent.train\u001b[0;34m(self, max_episode_num, env, behavior_name)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_learn(torch\u001b[38;5;241m.\u001b[39mtensor(states, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    183\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    184\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mtensor(y_i, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# update Actor and return policy loss\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# store the performance of the algorithm.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[228], line 80\u001b[0m, in \u001b[0;36mSACagent.actor_learn\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactor_learn\u001b[39m(\u001b[38;5;28mself\u001b[39m, states):\n\u001b[0;32m---> 80\u001b[0m     mu, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     actions, log_pdfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39msample_normal(mu, std)\n\u001b[1;32m     82\u001b[0m     log_pdfs \u001b[38;5;241m=\u001b[39m log_pdfs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[226], line 17\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh2(x))\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh3(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "# usually 30K is enough.\n",
    "agent.train(30000, env, behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
